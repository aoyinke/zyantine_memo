#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„DeepSeekå‚æ•°è½¬æ¢éªŒè¯è„šæœ¬
ä¸ä¾èµ–é¡¹ç›®å…¶ä»–æ¨¡å—ï¼Œç›´æ¥éªŒè¯æ ¸å¿ƒä¿®å¤é€»è¾‘
"""
import logging
from unittest.mock import MagicMock, patch
from enum import Enum

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ¨¡æ‹Ÿå¿…è¦çš„ç±»å’Œæšä¸¾
class LLMProvider(Enum):
    """æ¨¡æ‹ŸLLMProvideræšä¸¾"""
    OPENAI = "openai"
    DEEPSEEK = "deepseek"
    
class LLMModelConfig:
    """æ¨¡æ‹ŸLLMModelConfigç±»"""
    def __init__(self, provider, model_name, api_key, base_url, timeout, max_retries, use_max_completion_tokens):
        self.provider = provider
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        self.use_max_completion_tokens = use_max_completion_tokens

# æ¨¡æ‹ŸOpenAICompatibleServiceçš„æ ¸å¿ƒé€»è¾‘
class MockOpenAICompatibleService:
    """æ¨¡æ‹ŸOpenAICompatibleServiceï¼ŒåªåŒ…å«æ ¸å¿ƒçš„å‚æ•°è½¬æ¢é€»è¾‘"""
    def __init__(self, config):
        self.config = config
        self.client = None
    
    def _call_api(self, messages, max_tokens, temperature, stream, request_id):
        """æ¨¡æ‹Ÿ_call_apiæ–¹æ³•ï¼ŒåŒ…å«å‚æ•°è½¬æ¢é€»è¾‘"""
        if self.config.use_max_completion_tokens:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                max_completion_tokens=max_tokens,  # DeepSeekä½¿ç”¨max_completion_tokens
                temperature=temperature,
                stream=stream
            )
        else:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                max_tokens=max_tokens,  # OpenAIä½¿ç”¨max_tokens
                temperature=temperature,
                stream=stream
            )
        return response

def verify_deepseek_param_conversion():
    """
    éªŒè¯DeepSeekå‚æ•°è½¬æ¢é€»è¾‘
    """
    logger.info("=== éªŒè¯DeepSeekå‚æ•°è½¬æ¢é€»è¾‘ ===")
    
    # åˆ›å»ºDeepSeeké…ç½®
    deepseek_config = LLMModelConfig(
        provider=LLMProvider.DEEPSEEK,
        model_name="deepseek-chat",
        api_key="test_key",
        base_url="https://api.deepseek.com",
        timeout=30,
        max_retries=3,
        use_max_completion_tokens=True  # å¯ç”¨max_completion_tokens
    )
    
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    service = MockOpenAICompatibleService(deepseek_config)
    
    # æ¨¡æ‹Ÿå®¢æˆ·ç«¯
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_client.chat.completions.create.return_value = mock_response
    service.client = mock_client
    
    # æµ‹è¯•APIè°ƒç”¨
    messages = [{"role": "system", "content": "You are a helpful assistant."}, 
               {"role": "user", "content": "Hello, DeepSeek!"}]
    
    service._call_api(
        messages=messages,
        max_tokens=500,
        temperature=0.7,
        stream=False,
        request_id="test-request-123"
    )
    
    # æ£€æŸ¥APIè°ƒç”¨å‚æ•°
    call_args = mock_client.chat.completions.create.call_args
    kwargs = call_args.kwargs
    
    logger.info("APIè°ƒç”¨å‚æ•°:")
    for key, value in kwargs.items():
        logger.info(f"   - {key}: {value}")
    
    # éªŒè¯å‚æ•°è½¬æ¢æ˜¯å¦æ­£ç¡®
    if "max_completion_tokens" in kwargs and "max_tokens" not in kwargs:
        logger.info("âœ… éªŒè¯é€šè¿‡ï¼DeepSeekæ­£ç¡®ä½¿ç”¨äº†max_completion_tokenså‚æ•°")
        logger.info(f"   - max_completion_tokens: {kwargs['max_completion_tokens']}")
        return True
    else:
        logger.error("âŒ éªŒè¯å¤±è´¥ï¼DeepSeekæ²¡æœ‰æ­£ç¡®ä½¿ç”¨max_completion_tokenså‚æ•°")
        logger.error(f"   - ä½¿ç”¨äº†max_tokens: {'max_tokens' in kwargs}")
        logger.error(f"   - ä½¿ç”¨äº†max_completion_tokens: {'max_completion_tokens' in kwargs}")
        return False

def verify_openai_param_usage():
    """
    éªŒè¯OpenAIå‚æ•°ä½¿ç”¨é€»è¾‘ï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
    """
    logger.info("\n=== éªŒè¯OpenAIå‚æ•°ä½¿ç”¨é€»è¾‘ ===")
    
    # åˆ›å»ºOpenAIé…ç½®
    openai_config = LLMModelConfig(
        provider=LLMProvider.OPENAI,
        model_name="gpt-5-nano",
        api_key="test_key",
        base_url="https://api.openai.com/v1",
        timeout=30,
        max_retries=3,
        use_max_completion_tokens=False  # ç¦ç”¨max_completion_tokens
    )
    
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    service = MockOpenAICompatibleService(openai_config)
    
    # æ¨¡æ‹Ÿå®¢æˆ·ç«¯
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_client.chat.completions.create.return_value = mock_response
    service.client = mock_client
    
    # æµ‹è¯•APIè°ƒç”¨
    messages = [{"role": "system", "content": "You are a helpful assistant."}, 
               {"role": "user", "content": "Hello, OpenAI!"}]
    
    service._call_api(
        messages=messages,
        max_tokens=500,
        temperature=0.7,
        stream=False,
        request_id="test-request-456"
    )
    
    # æ£€æŸ¥APIè°ƒç”¨å‚æ•°
    call_args = mock_client.chat.completions.create.call_args
    kwargs = call_args.kwargs
    
    logger.info("APIè°ƒç”¨å‚æ•°:")
    for key, value in kwargs.items():
        logger.info(f"   - {key}: {value}")
    
    # éªŒè¯å‚æ•°ä½¿ç”¨æ˜¯å¦æ­£ç¡®
    if "max_tokens" in kwargs and "max_completion_tokens" not in kwargs:
        logger.info("âœ… éªŒè¯é€šè¿‡ï¼OpenAIæ­£ç¡®ä½¿ç”¨äº†max_tokenså‚æ•°")
        logger.info(f"   - max_tokens: {kwargs['max_tokens']}")
        return True
    else:
        logger.error("âŒ éªŒè¯å¤±è´¥ï¼OpenAIæ²¡æœ‰æ­£ç¡®ä½¿ç”¨max_tokenså‚æ•°")
        logger.error(f"   - ä½¿ç”¨äº†max_tokens: {'max_tokens' in kwargs}")
        logger.error(f"   - ä½¿ç”¨äº†max_completion_tokens: {'max_completion_tokens' in kwargs}")
        return False

# éªŒè¯llm_service_factoryä¸­çš„é»˜è®¤è¡Œä¸º
def verify_factory_default_behavior():
    """
    éªŒè¯LLMServiceFactoryå¯¹DeepSeekçš„é»˜è®¤è¡Œä¸º
    """
    logger.info("\n=== éªŒè¯LLMServiceFactoryé»˜è®¤è¡Œä¸º ===")
    
    # æ¨¡æ‹Ÿå·¥å‚åˆ›å»ºæœåŠ¡çš„é€»è¾‘
    def mock_create_service(provider, config):
        """æ¨¡æ‹Ÿcreate_serviceæ–¹æ³•çš„æ ¸å¿ƒé€»è¾‘"""
        provider_enum = LLMProvider(provider)
        
        # æ³¨æ„è¿™é‡Œçš„å…³é”®è¡Œï¼šå¯¹äºDeepSeekï¼Œé»˜è®¤å¯ç”¨use_max_completion_tokens
        use_max_completion_tokens = config.get("use_max_completion_tokens", provider_enum == LLMProvider.DEEPSEEK)
        
        model_config = LLMModelConfig(
            provider=provider_enum,
            model_name=config.get("chat_model", "default-model"),
            api_key=config.get("api_key", ""),
            base_url=config.get("base_url", ""),
            timeout=config.get("timeout", 30),
            max_retries=config.get("max_retries", 3),
            use_max_completion_tokens=use_max_completion_tokens
        )
        
        return model_config
    
    # æµ‹è¯•DeepSeeké…ç½®
    deepseek_config = {
        "chat_model": "deepseek-chat",
        "api_key": "test_key",
        "base_url": "https://api.deepseek.com",
    }
    
    model_config = mock_create_service("deepseek", deepseek_config)
    
    if model_config.use_max_completion_tokens is True:
        logger.info("âœ… éªŒè¯é€šè¿‡ï¼LLMServiceFactoryä¸ºDeepSeeké»˜è®¤å¯ç”¨äº†use_max_completion_tokens")
        logger.info(f"   - provider: {model_config.provider.value}")
        logger.info(f"   - use_max_completion_tokens: {model_config.use_max_completion_tokens}")
        return True
    else:
        logger.error("âŒ éªŒè¯å¤±è´¥ï¼LLMServiceFactoryæ²¡æœ‰ä¸ºDeepSeeké»˜è®¤å¯ç”¨use_max_completion_tokens")
        return False

if __name__ == "__main__":
    logger.info("å¼€å§‹éªŒè¯DeepSeekå‚æ•°è½¬æ¢ä¿®å¤...")
    
    # è¿è¡ŒéªŒè¯
    deepseek_result = verify_deepseek_param_conversion()
    openai_result = verify_openai_param_usage()
    factory_result = verify_factory_default_behavior()
    
    logger.info("\n=== éªŒè¯æ€»ç»“ ===")
    logger.info(f"DeepSeekå‚æ•°è½¬æ¢: {'âœ… é€šè¿‡' if deepseek_result else 'âŒ å¤±è´¥'}")
    logger.info(f"OpenAIå‚æ•°ä½¿ç”¨: {'âœ… é€šè¿‡' if openai_result else 'âŒ å¤±è´¥'}")
    logger.info(f"å·¥å‚é»˜è®¤è¡Œä¸º: {'âœ… é€šè¿‡' if factory_result else 'âŒ å¤±è´¥'}")
    
    if deepseek_result and openai_result and factory_result:
        logger.info("\nğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼DeepSeekå‚æ•°è½¬æ¢ä¿®å¤é€»è¾‘æ­£ç¡®")
        logger.info("\nä¿®å¤æ€»ç»“ï¼š")
        logger.info("1. å¯ç”¨äº†DeepSeeké…ç½® (config_manager.py)")
        logger.info("2. OpenAICompatibleServiceæ ¹æ®use_max_completion_tokenså‚æ•°é€‰æ‹©ä½¿ç”¨çš„APIå‚æ•°")
        logger.info("3. LLMServiceFactoryä¸ºDeepSeeké»˜è®¤å¯ç”¨use_max_completion_tokens")
        logger.info("4. å½“use_max_completion_tokens=Trueæ—¶ï¼ŒAPIè°ƒç”¨ä½¿ç”¨max_completion_tokenså‚æ•°")
        logger.info("\nè¿™å°†è§£å†³'unsupported parameter: max_tokens'é”™è¯¯")
    else:
        logger.error("\nğŸ’¥ éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
