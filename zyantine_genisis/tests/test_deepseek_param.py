#!/usr/bin/env python3
"""
DeepSeekå‚æ•°è½¬æ¢æµ‹è¯•è„šæœ¬
éªŒè¯OpenAICompatibleServiceæ˜¯å¦æ­£ç¡®å°†max_tokensè½¬æ¢ä¸ºmax_completion_tokens
"""
import sys
import os
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath('/Users/gyc/Desktop/GYC_coding/zyantine/zyantine_memo')
sys.path.append(project_root)

from zyantine_genisis.api.llm_service_factory import LLMServiceFactory
from zyantine_genisis.api.llm_service import OpenAICompatibleService
from zyantine_genisis.api.llm_provider import LLMProvider, LLMModelConfig
from zyantine_genisis.utils.logger import SystemLogger

# é…ç½®æ—¥å¿—
logger = SystemLogger().get_logger("deepseek_param_test", level=logging.DEBUG)

def test_deepseek_param_conversion():
    """æµ‹è¯•DeepSeekå‚æ•°è½¬æ¢é€»è¾‘"""
    logger.info("å¼€å§‹DeepSeekå‚æ•°è½¬æ¢æµ‹è¯•...")
    
    # 1. åˆ›å»ºDeepSeeké…ç½®
    deepseek_config = {
        "api_key": "test_key",  # æµ‹è¯•ç”¨ï¼Œå®é™…è°ƒç”¨ä¼šå¤±è´¥ä½†èƒ½çœ‹åˆ°å‚æ•°
        "base_url": "https://api.deepseek.com",
        "chat_model": "deepseek-chat",
        "timeout": 30,
        "max_retries": 3,
        "enabled": True
    }
    
    # 2. é€šè¿‡å·¥å‚åˆ›å»ºæœåŠ¡
    logger.info("é€šè¿‡LLMServiceFactoryåˆ›å»ºDeepSeekæœåŠ¡...")
    service = LLMServiceFactory.create_service("deepseek", deepseek_config)
    
    if not service:
        logger.error("åˆ›å»ºDeepSeekæœåŠ¡å¤±è´¥")
        return False
    
    logger.info(f"æˆåŠŸåˆ›å»ºæœåŠ¡: {type(service).__name__}")
    logger.info(f"æœåŠ¡æä¾›å•†: {service.provider.value}")
    logger.info(f"æ¨¡å‹åç§°: {service.model}")
    logger.info(f"base_url: {service.base_url}")
    logger.info(f"use_max_completion_tokens: {service.config.use_max_completion_tokens}")
    
    # 3. éªŒè¯å‚æ•°è½¬æ¢é€»è¾‘
    try:
        # æ¨¡æ‹ŸAPIè°ƒç”¨ï¼ˆä¼šå¤±è´¥ï¼Œä½†èƒ½çœ‹åˆ°å‚æ•°ï¼‰
        logger.info("æµ‹è¯•APIè°ƒç”¨å‚æ•°...")
        
        # é‡å†™clientæ–¹æ³•ä»¥æ•è·å‚æ•°
        original_create = service.client.chat.completions.create
        
        def mock_create(**kwargs):
            logger.info(f"APIè°ƒç”¨å‚æ•°: {kwargs}")
            if "max_completion_tokens" in kwargs:
                logger.info("âœ… æˆåŠŸ: ä½¿ç”¨äº†max_completion_tokenså‚æ•°")
            elif "max_tokens" in kwargs:
                logger.error("âŒ å¤±è´¥: ä½¿ç”¨äº†max_tokenså‚æ•°")
            else:
                logger.error("âŒ å¤±è´¥: æ²¡æœ‰max_tokensæˆ–max_completion_tokenså‚æ•°")
            
            # å¼•å‘å¼‚å¸¸æ¨¡æ‹ŸAPIè°ƒç”¨å¤±è´¥
            raise Exception("æµ‹è¯•å¼‚å¸¸: æ¨¡æ‹ŸAPIè°ƒç”¨")
        
        service.client.chat.completions.create = mock_create
        
        # å°è¯•è°ƒç”¨API
        service.generate_reply(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹",
            user_input="æµ‹è¯•",
            max_tokens=100,
            temperature=0.7,
            stream=False
        )
        
    except Exception as e:
        # é¢„æœŸä¼šå¤±è´¥ï¼Œå› ä¸ºæ˜¯æµ‹è¯•
        if "æµ‹è¯•å¼‚å¸¸" in str(e):
            logger.info("æµ‹è¯•å®Œæˆï¼Œå‚æ•°éªŒè¯æˆåŠŸ")
            return True
        else:
            logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
            return False
    finally:
        # æ¢å¤åŸå§‹æ–¹æ³•
        service.client.chat.completions.create = original_create

def test_openai_service():
    """æµ‹è¯•OpenAIæœåŠ¡ï¼ˆå¯¹æ¯”ç”¨ï¼‰"""
    logger.info("\nå¼€å§‹OpenAIæœåŠ¡æµ‹è¯•ï¼ˆå¯¹æ¯”ç”¨ï¼‰...")
    
    openai_config = {
        "api_key": "test_key",
        "base_url": "https://api.openai.com/v1",
        "chat_model": "gpt-5-nano-2025-08-07",
        "timeout": 30,
        "max_retries": 3
    }
    
    service = LLMServiceFactory.create_service("openai", openai_config)
    
    if not service:
        logger.error("åˆ›å»ºOpenAIæœåŠ¡å¤±è´¥")
        return False
    
    logger.info(f"æˆåŠŸåˆ›å»ºæœåŠ¡: {type(service).__name__}")
    logger.info(f"use_max_completion_tokens: {service.config.use_max_completion_tokens}")
    
    try:
        # é‡å†™clientæ–¹æ³•
        original_create = service.client.chat.completions.create
        
        def mock_create(**kwargs):
            logger.info(f"OpenAI APIè°ƒç”¨å‚æ•°: {kwargs}")
            if "max_tokens" in kwargs:
                logger.info("âœ… æˆåŠŸ: OpenAIä½¿ç”¨äº†max_tokenså‚æ•°")
            else:
                logger.error("âŒ å¤±è´¥: OpenAIæ²¡æœ‰ä½¿ç”¨max_tokenså‚æ•°")
            raise Exception("æµ‹è¯•å¼‚å¸¸: æ¨¡æ‹ŸAPIè°ƒç”¨")
        
        service.client.chat.completions.create = mock_create
        
        # å°è¯•è°ƒç”¨API
        service.generate_reply(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹",
            user_input="æµ‹è¯•",
            max_tokens=100,
            temperature=0.7,
            stream=False
        )
        
    except Exception as e:
        if "æµ‹è¯•å¼‚å¸¸" in str(e):
            logger.info("OpenAIæµ‹è¯•å®Œæˆ")
            return True
        else:
            logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
            return False
    finally:
        service.client.chat.completions.create = original_create

if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("DeepSeekå‚æ•°è½¬æ¢æµ‹è¯•")
    logger.info("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    deepseek_result = test_deepseek_param_conversion()
    openai_result = test_openai_service()
    
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•æ€»ç»“:")
    logger.info(f"DeepSeekå‚æ•°è½¬æ¢: {'âœ… æˆåŠŸ' if deepseek_result else 'âŒ å¤±è´¥'}")
    logger.info(f"OpenAIå‚æ•°éªŒè¯: {'âœ… æˆåŠŸ' if openai_result else 'âŒ å¤±è´¥'}")
    
    if deepseek_result and openai_result:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å‚æ•°è½¬æ¢é€»è¾‘æ­£å¸¸å·¥ä½œ")
        sys.exit(0)
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥å‚æ•°è½¬æ¢é€»è¾‘")
        sys.exit(1)
