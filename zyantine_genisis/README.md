# ZyantineAI - è‡ªè¡ä½“AIç³»ç»Ÿ

ä¸€ä¸ªå…·æœ‰è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œæ”¯æŒå¤šLLMæä¾›å•†ã€è®°å¿†ç®¡ç†ã€è®¤çŸ¥æµç¨‹å’Œåè®®å¼•æ“ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¤– å¤šLLMæä¾›å•†æ”¯æŒ
- **OpenAI**: GPTç³»åˆ—æ¨¡å‹
- **DeepSeek**: æ·±åº¦æ±‚ç´¢å¤§æ¨¡å‹
- **Anthropic**: Claudeç³»åˆ—æ¨¡å‹
- **æ™ºè°±AI**: GLMç³»åˆ—æ¨¡å‹
- **æœˆä¹‹æš—é¢**: Moonshotç³»åˆ—æ¨¡å‹
- **é˜¿é‡Œäº‘**: é€šä¹‰åƒé—®ç³»åˆ—æ¨¡å‹
- **ç™¾åº¦æ–‡å¿ƒ**: æ–‡å¿ƒä¸€è¨€ç³»åˆ—æ¨¡å‹

### ğŸ§  æ™ºèƒ½è®°å¿†ç³»ç»Ÿ
- **Memo0è®°å¿†ç³»ç»Ÿ**: é«˜æ•ˆçš„è¯­ä¹‰è®°å¿†å­˜å‚¨å’Œæ£€ç´¢
- **åˆ†å±‚å­˜å‚¨ä¼˜åŒ–**: è‡ªåŠ¨ç®¡ç†ä¸åŒå±‚çº§çš„è®°å¿†å­˜å‚¨
- **è®°å¿†å»é‡**: æ™ºèƒ½æ£€æµ‹å’Œå»é™¤é‡å¤è®°å¿†
- **å®‰å…¨ç®¡ç†**: æ”¯æŒåŠ å¯†å’Œè®¿é—®æ§åˆ¶
- **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§è®°å¿†ç³»ç»Ÿæ€§èƒ½

### ğŸ”„ è®¤çŸ¥æµç¨‹ç®¡ç†
- **æ ¸å¿ƒèº«ä»½è¯†åˆ«**: ç»´æŒä¸€è‡´çš„ç³»ç»Ÿäººæ ¼
- **è®¤çŸ¥æµç¨‹ç¼–æ’**: å¤šé˜¶æ®µè®¤çŸ¥å¤„ç†æµç¨‹
- **å…ƒè®¤çŸ¥èƒ½åŠ›**: è‡ªæˆ‘åæ€å’Œè°ƒæ•´
- **æ¬²æœ›å¼•æ“**: åŠ¨æ€è°ƒæ•´ç³»ç»Ÿç›®æ ‡å’ŒåŠ¨æœº

### ğŸ›¡ï¸ åè®®å¼•æ“
- **äº‹å®æ£€æŸ¥**: ç¡®ä¿å›ç­”çš„å‡†ç¡®æ€§
- **é•¿åº¦æ§åˆ¶**: è‡ªåŠ¨è°ƒæ•´å“åº”é•¿åº¦
- **è¡¨è¾¾åè®®**: è§„èŒƒåŒ–è¡¨è¾¾æ–¹å¼
- **å†²çªè§£å†³**: å¤„ç†åè®®é—´çš„å†²çª

### ğŸŒ OpenAIå…¼å®¹API
- **æ ‡å‡†APIæ ¼å¼**: å®Œå…¨å…¼å®¹OpenAI Chat Completions API
- **æµå¼å“åº”**: æ”¯æŒå®æ—¶æµå¼è¾“å‡º
- **å¤šè½®å¯¹è¯**: è‡ªåŠ¨ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡
- **å¥åº·æ£€æŸ¥**: å®Œæ•´çš„ç³»ç»Ÿç›‘æ§å’ŒçŠ¶æ€æŠ¥å‘Š

## ğŸ“¦ å®‰è£…

### ç¯å¢ƒè¦æ±‚
- Python 3.11+
- pip

### å®‰è£…ä¾èµ–

```bash
cd zyantine_genisis
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é…ç½®LLMæä¾›å•†

ç¼–è¾‘ `config/llm_config.json` æ–‡ä»¶ï¼Œé…ç½®ä½ æƒ³è¦ä½¿ç”¨çš„LLMæä¾›å•†ï¼š

```json
{
  "api": {
    "enabled": true,
    "provider": "deepseek",
    "api_key": "your-api-key-here",
    "base_url": "https://api.deepseek.com",
    "chat_model": "deepseek-chat",
    "embedding_model": "text-embedding-3-large",
    "providers": {
      "openai": {
        "enabled": false,
        "api_key": "your-openai-key",
        "base_url": "https://api.openai.com/v1",
        "chat_model": "gpt-5-nano-2025-08-07"
      },
      "deepseek": {
        "enabled": true,
        "api_key": "your-deepseek-key",
        "base_url": "https://api.deepseek.com",
        "chat_model": "deepseek-chat"
      }
    }
  }
}
```

### 2. å¯åŠ¨ç³»ç»Ÿ

#### äº¤äº’æ¨¡å¼

```bash
python main.py --interactive
```

#### APIæœåŠ¡æ¨¡å¼

```bash
python api_server.py
```

æœåŠ¡å¯åŠ¨åï¼Œè®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£ã€‚

#### æ‰¹é‡å¤„ç†æ¨¡å¼

```bash
python main.py --batch input.txt --output output.json
```

### 3. ä½¿ç”¨ç¤ºä¾‹

#### Pythonå®¢æˆ·ç«¯

```python
from zyantine_facade import create_zyantine

# åˆ›å»ºç³»ç»Ÿå®ä¾‹
facade = create_zyantine(
    api_key="your-api-key",
    session_id="user-123"
)

# å‘é€æ¶ˆæ¯
response = facade.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response)

# ä¿å­˜è®°å¿†
facade.save_memory()

# æŸ¥çœ‹çŠ¶æ€
status = facade.get_status()
print(status)
```

#### APIè°ƒç”¨

```python
import requests

# éæµå¼è¯·æ±‚
response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "zyantine-v1",
        "messages": [
            {"role": "user", "content": "ä½ å¥½"}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])

# æµå¼è¯·æ±‚
response = requests.post(
    "http://localhost:8000/v1/chat/completions/stream",
    json={
        "model": "zyantine-v1",
        "messages": [
            {"role": "user", "content": "ä½ å¥½"}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))
```

## ğŸ“– è¯¦ç»†æ–‡æ¡£

### LLMæä¾›å•†é…ç½®

#### åˆ‡æ¢åˆ°OpenAI

```json
{
  "api": {
    "provider": "openai",
    "api_key": "your-openai-key",
    "base_url": "https://api.openai.com/v1",
    "chat_model": "gpt-5-nano-2025-08-07",
    "providers": {
      "openai": {
        "enabled": true,
        "api_key": "your-openai-key",
        "base_url": "https://api.openai.com/v1",
        "chat_model": "gpt-5-nano-2025-08-07"
      },
      "deepseek": {
        "enabled": false
      }
    }
  }
}
```

#### ä½¿ç”¨è‡ªå®šä¹‰base_url

```json
{
  "api": {
    "provider": "openai",
    "base_url": "https://openkey.cloud/v1",
    "chat_model": "gpt-5-nano-2025-08-07"
  }
}
```

### è®°å¿†ç³»ç»Ÿé…ç½®

```json
{
  "memory": {
    "system_type": "memo0",
    "max_memories": 10000,
    "retrieval_limit": 5,
    "similarity_threshold": 0.7,
    "enable_semantic_cache": true,
    "cache_ttl": 300,
    "backup_interval": 3600,
    "backup_path": "./memory_backups"
  }
}
```

### è®¤çŸ¥æµç¨‹é…ç½®

```json
{
  "processing": {
    "mode": "standard",
    "enable_stage_parallelism": false,
    "max_conversation_history": 1000,
    "enable_real_time_analysis": true,
    "stage_configs": {
      "preprocess": {"enabled": true, "timeout": 5},
      "memory_retrieval": {"enabled": true, "cache_results": true},
      "desire_update": {"enabled": true, "update_frequency": "always"},
      "cognitive_flow": {"enabled": true, "max_iterations": 3},
      "reply_generation": {"enabled": true, "fallback_to_template": true},
      "protocol_review": {"enabled": true, "strict_mode": false}
    }
  }
}
```

### åè®®å¼•æ“é…ç½®

```json
{
  "protocols": {
    "enable_fact_check": true,
    "enable_length_regulation": true,
    "enable_expression_protocol": true,
    "fact_check_strictness": 0.8,
    "max_response_length": 2000,
    "min_response_length": 50,
    "allow_uncertainty_phrases": true
  }
}
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
# ä½¿ç”¨æµ‹è¯•è¿è¡Œå™¨
python run_tests.py --all

# æˆ–ç›´æ¥è¿è¡Œæµ‹è¯•æ–‡ä»¶
python tests/api/test_llm_provider.py
```

### è¿è¡Œç‰¹å®šç±»åˆ«çš„æµ‹è¯•

```bash
# è®°å¿†æ¨¡å—æµ‹è¯•
python run_tests.py --category memory

# è®¤çŸ¥æ¨¡å—æµ‹è¯•
python run_tests.py --category cognition

# åè®®æ¨¡å—æµ‹è¯•
python run_tests.py --category protocols

# APIæ¨¡å—æµ‹è¯•
python run_tests.py --category api

# ç³»ç»Ÿçº§æµ‹è¯•
python run_tests.py --category system
```

### è¿è¡Œå¿«é€Ÿæµ‹è¯•

```bash
python run_tests.py --quick
```

## ğŸ“Š é¡¹ç›®ç»“æ„

```
zyantine_genisis/
â”œâ”€â”€ api/                    # APIå’ŒLLMæœåŠ¡
â”‚   â”œâ”€â”€ llm_provider.py    # LLMæä¾›å•†æšä¸¾å’Œé…ç½®
â”‚   â”œâ”€â”€ llm_service.py     # LLMæœåŠ¡æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ llm_service_factory.py  # LLMæœåŠ¡å·¥å‚
â”‚   â”œâ”€â”€ openai_service.py  # OpenAIå…¼å®¹æœåŠ¡
â”‚   â””â”€â”€ service_provider.py  # æœåŠ¡æä¾›å•†ç®¡ç†
â”œâ”€â”€ cognition/             # è®¤çŸ¥æ¨¡å—
â”‚   â”œâ”€â”€ core_identity.py   # æ ¸å¿ƒèº«ä»½è¯†åˆ«
â”‚   â”œâ”€â”€ cognitive_flow_manager.py  # è®¤çŸ¥æµç¨‹ç®¡ç†
â”‚   â”œâ”€â”€ desire_engine.py   # æ¬²æœ›å¼•æ“
â”‚   â””â”€â”€ meta_cognition.py  # å…ƒè®¤çŸ¥
â”œâ”€â”€ config/                # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ config_manager.py  # é…ç½®ç®¡ç†å™¨
â”‚   â””â”€â”€ llm_config.json    # LLMé…ç½®æ–‡ä»¶
â”œâ”€â”€ core/                  # ç³»ç»Ÿæ ¸å¿ƒ
â”‚   â”œâ”€â”€ system_core.py     # ç³»ç»Ÿæ ¸å¿ƒ
â”‚   â”œâ”€â”€ processing_pipeline.py  # å¤„ç†ç®¡é“
â”‚   â””â”€â”€ stage_handlers.py  # é˜¶æ®µå¤„ç†å™¨
â”œâ”€â”€ memory/                # è®°å¿†ç³»ç»Ÿ
â”‚   â”œâ”€â”€ memory_manager.py  # è®°å¿†ç®¡ç†å™¨
â”‚   â””â”€â”€ memory_store.py    # è®°å¿†å­˜å‚¨
â”œâ”€â”€ protocols/             # åè®®å¼•æ“
â”‚   â”œâ”€â”€ fact_checker.py    # äº‹å®æ£€æŸ¥
â”‚   â”œâ”€â”€ length_regulator.py # é•¿åº¦æ§åˆ¶
â”‚   â””â”€â”€ expression_validator.py # è¡¨è¾¾éªŒè¯
â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ logger.py          # æ—¥å¿—å·¥å…·
â”‚   â”œâ”€â”€ metrics.py         # æŒ‡æ ‡æ”¶é›†
â”‚   â””â”€â”€ error_handler.py   # é”™è¯¯å¤„ç†
â”œâ”€â”€ examples/              # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ llm_provider_usage.py
â”‚   â””â”€â”€ memory_demo.py
â”œâ”€â”€ tests/                 # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ api/               # APIæµ‹è¯•
â”‚   â”œâ”€â”€ cognition/         # è®¤çŸ¥æµ‹è¯•
â”‚   â”œâ”€â”€ memory/            # è®°å¿†æµ‹è¯•
â”‚   â””â”€â”€ protocols/         # åè®®æµ‹è¯•
â”œâ”€â”€ main.py                # ä¸»å…¥å£
â”œâ”€â”€ api_server.py          # APIæœåŠ¡å™¨
â”œâ”€â”€ zyantine_facade.py     # å¤–è§‚æ¨¡å¼å…¥å£
â””â”€â”€ README.md              # æœ¬æ–‡ä»¶
```

## ğŸ”§ å‘½ä»¤è¡Œå‚æ•°

### main.py

```bash
python main.py [OPTIONS]

é€‰é¡¹:
  --config, -c PATH        é…ç½®æ–‡ä»¶è·¯å¾„
  --api-key, -k KEY        OpenAI APIå¯†é’¥
  --session, -s ID         ä¼šè¯ID (é»˜è®¤: default)
  --interactive, -i        äº¤äº’æ¨¡å¼
  --batch, -b FILE         æ‰¹é‡å¤„ç†è¾“å…¥æ–‡ä»¶
  --output, -o FILE        æ‰¹é‡å¤„ç†è¾“å‡ºæ–‡ä»¶
  --profile, -p FILE       ç”¨æˆ·é…ç½®æ–‡ä»¶
  --self-profile, -sp FILE è‡ªè¡ä½“é…ç½®æ–‡ä»¶
  --status                 æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
  --save                   ä¿å­˜è®°å¿†ç³»ç»Ÿ
  --cleanup                æ¸…ç†è®°å¿†
  --log-level LEVEL        æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)
```

### api_server.py

```bash
python api_server.py [OPTIONS]

é€‰é¡¹:
  --host HOST              ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)
  --port PORT              ç›‘å¬ç«¯å£ (é»˜è®¤: 8000)
  --api-key KEY            OpenAI APIå¯†é’¥
  --session ID             ä¼šè¯ID (é»˜è®¤: default)
```

## ğŸŒŸ æ ¸å¿ƒæ¦‚å¿µ

### å¤–è§‚æ¨¡å¼ (Facade Pattern)

`ZyantineFacade` ç±»æä¾›äº†ç®€åŒ–çš„ç³»ç»Ÿæ¥å£ï¼Œéšè—äº†å¤æ‚çš„å†…éƒ¨å®ç°ï¼š

```python
from zyantine_facade import create_zyantine

facade = create_zyantine(api_key="your-key", session_id="user-123")
response = facade.chat("ä½ å¥½")
```

### å·¥å‚æ¨¡å¼ (Factory Pattern)

`LLMServiceFactory` è´Ÿè´£åˆ›å»ºä¸åŒæä¾›å•†çš„LLMæœåŠ¡ï¼š

```python
from api.llm_service_factory import LLMServiceFactory

service = LLMServiceFactory.create_service("deepseek", config)
```

### è®°å¿†ç”Ÿå‘½å‘¨æœŸ

è®°å¿†ç³»ç»Ÿæ”¯æŒå®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼š
- åˆ›å»º â†’ å­˜å‚¨ â†’ æ£€ç´¢ â†’ æ›´æ–° â†’ å½’æ¡£ â†’ åˆ é™¤

### è®¤çŸ¥æµç¨‹

æ ‡å‡†è®¤çŸ¥æµç¨‹åŒ…æ‹¬ä»¥ä¸‹é˜¶æ®µï¼š
1. é¢„å¤„ç† (Preprocess)
2. è®°å¿†æ£€ç´¢ (Memory Retrieval)
3. æ¬²æœ›æ›´æ–° (Desire Update)
4. è®¤çŸ¥æµç¨‹ (Cognitive Flow)
5. å›å¤ç”Ÿæˆ (Reply Generation)
6. åè®®å®¡æŸ¥ (Protocol Review)

## ğŸ” ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—æ–‡ä»¶

æ—¥å¿—æ–‡ä»¶å­˜å‚¨åœ¨ `logs/` ç›®å½•ä¸‹ï¼š
- `zyantine.log`: ä¸»ç³»ç»Ÿæ—¥å¿—
- `facade_*.log`: å¤–è§‚æ¨¡å¼æ—¥å¿—
- `api_service_provider_*.log`: APIæœåŠ¡æ—¥å¿—
- `llm_service_*.log`: LLMæœåŠ¡æ—¥å¿—
- å…¶ä»–æ¨¡å—æ—¥å¿—...

### å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8000/health
```

### ç³»ç»ŸçŠ¶æ€

```python
status = facade.get_status()
print(json.dumps(status, ensure_ascii=False, indent=2))
```

## ğŸ¤ ä¸è¯­éŸ³RTCé¡¹ç›®é›†æˆ

### é›†æˆæµç¨‹

1. **è¯­éŸ³è¯†åˆ«**: RTCé¡¹ç›®è¯†åˆ«ç”¨æˆ·è¯­éŸ³ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬
2. **APIè°ƒç”¨**: å°†è¯†åˆ«çš„æ–‡æœ¬å‘é€åˆ° `/v1/chat/completions` ç«¯ç‚¹
3. **è·å–å“åº”**: æ¥æ”¶AIç”Ÿæˆçš„æ–‡æœ¬å“åº”
4. **è¯­éŸ³åˆæˆ**: å°†å“åº”æ–‡æœ¬ä¼ é€’ç»™RTCé¡¹ç›®çš„è¯­éŸ³åˆæˆæ¨¡å—

### æµå¼å“åº”é›†æˆï¼ˆæ¨èï¼‰

```python
import requests
import json

def process_voice_input_stream(text: str, callback):
    """å¤„ç†è¯­éŸ³è¾“å…¥å¹¶æµå¼è¿”å›AIå“åº”"""
    response = requests.post(
        "http://localhost:8000/v1/chat/completions/stream",
        json={
            "model": "zyantine-v1",
            "messages": [
                {"role": "user", "content": text}
            ],
            "temperature": 0.7,
            "max_tokens": 1000
        },
        stream=True,
        timeout=30
    )
    
    full_response = ""
    for line in response.iter_lines():
        if line:
            line_str = line.decode('utf-8')
            if line_str == "data: [DONE]":
                break
            
            if line_str.startswith("data: "):
                data = json.loads(line_str[6:])
                delta = data["choices"][0].get("delta", {})
                content = delta.get("content", "")
                
                if content:
                    full_response += content
                    callback(content)  # å°†å†…å®¹ç‰‡æ®µä¼ é€’ç»™å›è°ƒå‡½æ•°
    
    return full_response

# ä½¿ç”¨ç¤ºä¾‹
def on_response_chunk(chunk: str):
    """å¤„ç†å“åº”ç‰‡æ®µçš„å›è°ƒå‡½æ•°"""
    print(chunk, end='', flush=True)
    # åœ¨è¿™é‡Œå¯ä»¥è°ƒç”¨è¯­éŸ³åˆæˆæ¨¡å—

user_text = "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
process_voice_input_stream(user_text, on_response_chunk)
```

## ğŸ“ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„LLMæä¾›å•†

1. åœ¨ `api/llm_provider.py` ä¸­æ·»åŠ æ–°çš„æšä¸¾å€¼
2. åœ¨ `api/llm_service.py` ä¸­å®ç°æ–°çš„æœåŠ¡ç±»
3. åœ¨ `api/llm_service_factory.py` ä¸­æ·»åŠ å·¥å‚æ–¹æ³•
4. åœ¨ `config/llm_config.json` ä¸­æ·»åŠ é…ç½®

### æ·»åŠ æ–°çš„è®¤çŸ¥é˜¶æ®µ

1. åœ¨ `core/stage_handlers.py` ä¸­å®ç°æ–°çš„å¤„ç†å™¨
2. åœ¨ `config/llm_config.json` ä¸­æ·»åŠ é˜¶æ®µé…ç½®
3. æ›´æ–°å¤„ç†ç®¡é“ä»¥åŒ…å«æ–°é˜¶æ®µ

### æ·»åŠ æ–°çš„åè®®

1. åœ¨ `protocols/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„åè®®ç±»
2. åœ¨ `protocols/protocol_engine.py` ä¸­æ³¨å†Œåè®®
3. åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨åè®®

## ğŸ› æ•…éšœæ’æŸ¥

### æœåŠ¡æ— æ³•å¯åŠ¨

1. æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨ï¼š`lsof -i :8000`
2. æ£€æŸ¥ä¾èµ–æ˜¯å¦å®Œæ•´ï¼š`pip install -r requirements.txt`
3. æŸ¥çœ‹æ—¥å¿—è¾“å‡º

### APIè°ƒç”¨å¤±è´¥

1. æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼šè®¿é—® http://localhost:8000/health
2. æ£€æŸ¥è¯·æ±‚æ ¼å¼æ˜¯å¦æ­£ç¡®
3. æŸ¥çœ‹æœåŠ¡æ—¥å¿—

### LLMæä¾›å•†è¿æ¥å¤±è´¥

1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥base_urlæ˜¯å¦æ­£ç¡®
3. æ£€æŸ¥ç½‘ç»œè¿æ¥
4. æŸ¥çœ‹LLMæœåŠ¡æ—¥å¿—

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸæœ‰é¡¹ç›®çš„è®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ã€‚

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- æäº¤Issue
- å‘é€Pull Request
- å‚ä¸è®¨è®º

---

**ZyantineAI** - è®©AIçœŸæ­£å…·æœ‰è‡ªæˆ‘è¿›åŒ–èƒ½åŠ› ğŸš€
