#!/usr/bin/env python3
"""
æµ‹è¯•APIç©ºå“åº”å¤„ç†
éªŒè¯å½“APIè¿”å›ç©ºå“åº”æ—¶ï¼Œç³»ç»Ÿæ˜¯å¦èƒ½æ­£ç¡®è§¦å‘é™çº§ç­–ç•¥
"""
import sys
import os
import json
import logging
from unittest.mock import MagicMock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ç›¸å…³æ¨¡å—
from api.openai_service import OpenAIService
from api.reply_generator import APIBasedReplyGenerator
from api.fallback_strategy import FallbackStrategy
from api.prompt_engine import PromptEngine
from utils.logger import get_logger

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = get_logger("test_empty_response")

def test_openai_service_empty_response():
    """æµ‹è¯•OpenAIServiceå¯¹ç©ºå“åº”çš„å¤„ç†"""
    logger.info("=== æµ‹è¯•OpenAIServiceç©ºå“åº”å¤„ç† ===")
    
    # åˆ›å»ºOpenAIServiceå®ä¾‹
    openai_service = OpenAIService(
        api_key="test_key",
        base_url="https://api.openai.com/v1",
        model="gpt-3.5-turbo"
    )
    
    # æ¨¡æ‹ŸAPIè¿”å›ç©ºå“åº”
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message.content = ""
    mock_response.usage = MagicMock()
    mock_response.usage.total_tokens = 10
    mock_response.usage.prompt_tokens = 5
    mock_response.usage.completion_tokens = 5
    
    # æµ‹è¯•generate_replyæ–¹æ³•
    with patch.object(openai_service.client.chat.completions, 'create', return_value=mock_response):
        reply, metadata = openai_service.generate_reply(
            system_prompt="æµ‹è¯•ç³»ç»Ÿæç¤º",
            user_input="æµ‹è¯•ç”¨æˆ·è¾“å…¥",
            conversation_history=None,
            max_tokens=500,
            temperature=0.7,
            stream=False
        )
    
    logger.info(f"OpenAIServiceè¿”å›: reply={reply}, metadata={metadata}")
    
    # éªŒè¯ç»“æœ
    assert reply is None, f"æœŸæœ›è¿”å›Noneï¼Œä½†å¾—åˆ°äº†: {reply}"
    assert metadata is None, f"æœŸæœ›è¿”å›Noneï¼Œä½†å¾—åˆ°äº†: {metadata}"
    
    logger.info("âœ… OpenAIServiceç©ºå“åº”å¤„ç†æµ‹è¯•é€šè¿‡")

def test_reply_generator_fallback():
    """æµ‹è¯•ReplyGeneratoråœ¨APIè¿”å›ç©ºæ—¶çš„é™çº§ç­–ç•¥"""
    logger.info("\n=== æµ‹è¯•ReplyGeneratoré™çº§ç­–ç•¥ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„APIæœåŠ¡
    mock_api_service = MagicMock()
    mock_api_service.is_available.return_value = True
    mock_api_service.generate_reply.return_value = (None, None)  # æ¨¡æ‹ŸAPIè¿”å›ç©º
    
    # åˆ›å»ºFallbackStrategyå’ŒPromptEngineå®ä¾‹
    fallback_strategy = FallbackStrategy()
    # ä¸ºPromptEngineæä¾›å¿…è¦çš„configå‚æ•°
    mock_config = MagicMock()
    prompt_engine = PromptEngine(config=mock_config)
    
    # åˆ›å»ºReplyGeneratorå®ä¾‹
    reply_generator = APIBasedReplyGenerator(
        api_service=mock_api_service,
        prompt_engine=prompt_engine,
        fallback_strategy=fallback_strategy
    )
    
    # æµ‹è¯•ç”Ÿæˆå›å¤
    test_context = {
        "user_input": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "action_plan": {"chosen_mask": "çŸ¥å·±", "primary_strategy": "empathy"},
        "growth_result": {},
        "context_analysis": {"topic_complexity": "low"},
        "conversation_history": [],
        "current_vectors": {"TR": 0.5, "CS": 0.5, "SA": 0.5},
        "memory_context": None
    }
    
    reply = reply_generator._generate_with_legacy_api(**test_context)
    
    logger.info(f"ReplyGeneratorç”Ÿæˆçš„å›å¤: {reply}")
    
    # éªŒè¯ç»“æœ
    assert reply is not None, "æœŸæœ›ç”Ÿæˆéç©ºå›å¤"
    assert reply != "", "æœŸæœ›ç”Ÿæˆéç©ºå­—ç¬¦ä¸²"
    
    logger.info(f"âœ… ReplyGeneratoré™çº§ç­–ç•¥æµ‹è¯•é€šè¿‡ï¼Œç”Ÿæˆäº†æœ‰æ•ˆå›å¤: {reply}")
    return reply

def test_integration_empty_response():
    """æµ‹è¯•å®Œæ•´æµç¨‹å¯¹ç©ºå“åº”çš„å¤„ç†"""
    logger.info("\n=== æµ‹è¯•å®Œæ•´æµç¨‹ç©ºå“åº”å¤„ç† ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„APIæœåŠ¡
    mock_api_service = MagicMock()
    mock_api_service.is_available.return_value = True
    mock_api_service.generate_reply.return_value = (None, None)  # æ¨¡æ‹ŸAPIè¿”å›ç©º
    
    # åˆ›å»ºFallbackStrategyå’ŒPromptEngineå®ä¾‹
    fallback_strategy = FallbackStrategy()
    # ä¸ºPromptEngineæä¾›å¿…è¦çš„configå‚æ•°
    mock_config = MagicMock()
    prompt_engine = PromptEngine(config=mock_config)
    
    # åˆ›å»ºReplyGeneratorå®ä¾‹
    reply_generator = APIBasedReplyGenerator(
        api_service=mock_api_service,
        prompt_engine=prompt_engine,
        fallback_strategy=fallback_strategy
    )
    
    # æµ‹è¯•ä»è®¤çŸ¥æµç¨‹ç”Ÿæˆå›å¤
    cognitive_result = {
        "final_action_plan": {"chosen_mask": "é•¿æœŸæ­æ¡£", "primary_strategy": "analysis"},
        "growth_result": {},
        "context_analysis": {"topic_complexity": "medium"},
        "user_input": "æˆ‘é‡åˆ°äº†ä¸€ä¸ªæŠ€æœ¯é—®é¢˜ï¼Œèƒ½å¸®æˆ‘åˆ†æä¸€ä¸‹å—ï¼Ÿ",
        "conversation_history": [],
        "current_vectors": {"TR": 0.6, "CS": 0.4, "SA": 0.3},
        "memory_context": None,
        "flow_id": "test-flow-123"
    }
    
    reply = reply_generator.generate_reply(cognitive_result=cognitive_result)
    
    logger.info(f"å®Œæ•´æµç¨‹ç”Ÿæˆçš„å›å¤: {reply}")
    
    # éªŒè¯ç»“æœ
    assert reply is not None, "æœŸæœ›ç”Ÿæˆéç©ºå›å¤"
    assert reply != "", "æœŸæœ›ç”Ÿæˆéç©ºå­—ç¬¦ä¸²"
    
    logger.info(f"âœ… å®Œæ•´æµç¨‹ç©ºå“åº”å¤„ç†æµ‹è¯•é€šè¿‡ï¼Œç”Ÿæˆäº†æœ‰æ•ˆå›å¤: {reply}")
    return reply

if __name__ == "__main__":
    try:
        logger.info("å¼€å§‹æµ‹è¯•APIç©ºå“åº”å¤„ç†æœºåˆ¶")
        
        test_openai_service_empty_response()
        test_reply_generator_fallback()
        test_integration_empty_response()
        
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼APIç©ºå“åº”å¤„ç†æœºåˆ¶å·¥ä½œæ­£å¸¸")
        sys.exit(0)
        
    except AssertionError as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
